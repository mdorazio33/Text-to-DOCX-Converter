{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692fd912",
   "metadata": {},
   "source": [
    "# Importing the Necessary Libraries\n",
    "\n",
    "The first step is to import the necessary libraries in order to use this extractor. This tool is used for extracting text from PDFs, Image-Only PDFs, and from a url leading to a webpage, so a variety of libraries are needed in order to for it to accommodate the aforementioned sources. Each of these packages must be installed prior to running this code in order for it to function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17906b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "from docx import Document\n",
    "import easyocr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import warnings\n",
    "from bs4 import Tag\n",
    "from PIL import Image, ImageEnhance\n",
    "from io import BytesIO\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80fcfe3",
   "metadata": {},
   "source": [
    "# Disabling User Warnings\n",
    "The second step is to disable user warnings, though this is done out of personal preference. Nevertheless, there are benefits to keeping them enabled as well, so if wish to keep them active, simply skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6389593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9ff70",
   "metadata": {},
   "source": [
    "# Defining Our Functions\n",
    "The third step is to define our functions. Ideally, we want our code to execute the same way every time. Therefore, the best way to do this is to wrap bits of our code in such a way that it does the same thing each time it is needed/called upon. This, in essence, is what defining our functions allows us to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5de45",
   "metadata": {},
   "source": [
    "### Function that Checks Pages for Text-Only Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd19175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_text_only_page(page):\n",
    "    # Code to check if the page contains only text (no images)\n",
    "    return not page.get_images(full=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6affcf2",
   "metadata": {},
   "source": [
    "### Function that Checks Pages for Image Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa23d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_page(page):\n",
    "    # Code to check if the page contains images\n",
    "    return bool(page.get_images(full=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46267d3c",
   "metadata": {},
   "source": [
    "### Function that Converts PDF Content to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba67075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    has_images = False\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text += page.get_text()\n",
    "\n",
    "        # Code to check if the page contains images\n",
    "        if not has_images and is_image_page(page):\n",
    "            has_images = True\n",
    "\n",
    "    doc.close()\n",
    "    return text, has_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c96932",
   "metadata": {},
   "source": [
    "### Function that Extracts Images from the PDF if Detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c19f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path):\n",
    "    images = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        images += page.get_images(full=True)\n",
    "\n",
    "    doc.close()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fac99",
   "metadata": {},
   "source": [
    "### Function that Preprocesses Extracted Images Using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb5a8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_opencv(image, resize_factor=10, desired_dpi=300):\n",
    "    # Code to resize the image by a factor of 10\n",
    "    resized_image = cv2.resize(image, None, fx=resize_factor, fy=resize_factor, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Code to convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Code to apply a Gaussian blur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "    # Code to apply adaptive thresholding to enhance contrast\n",
    "    _, threshold_image = cv2.threshold(blurred_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Code to apply morphological transformations to further clean the image\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    processed_image = cv2.morphologyEx(threshold_image, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "    # Code to adjust the intensity of black pixels in the processed image\n",
    "    processed_image = cv2.subtract(255, processed_image)\n",
    "\n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29be3f",
   "metadata": {},
   "source": [
    "### Function that Extracts Text from these Images Using EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9408f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image):\n",
    "    # Code used to load the preprocessed image\n",
    "    preprocessed_image = preprocess_image_opencv(image)\n",
    "\n",
    "    # Code used to enable easyocr for text extraction\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    result = reader.readtext(preprocessed_image)\n",
    "\n",
    "    # Code used to extract text from the result\n",
    "    text = ' '.join([entry[1] for entry in result])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f567e0",
   "metadata": {},
   "source": [
    "### Function that Extracts Text from Websites Via URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6611d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_website(url):\n",
    "    # Code used to fetch HTML content from the website\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Code used to enable BeautifulSoup to parse HTML and extract text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    def process_list_items(list_tag, level=0):\n",
    "        # Recursively process list items and maintain indentation\n",
    "        items = list_tag.find_all('li', recursive=False)\n",
    "        tags = []\n",
    "        for index, item in enumerate(items, start=1):\n",
    "            text = f\"{'  ' * level}{'â€¢ ' if list_tag.name == 'ul' else f'{index}. '}{item.get_text(strip=True)}\"\n",
    "            if item.find_all(['ul', 'ol']):\n",
    "                tags.extend(process_list_items(item, level + 1))\n",
    "            tag = Tag(name='text')\n",
    "            tag.string = text\n",
    "            tag.level = level\n",
    "            tags.append(tag)\n",
    "        return tags\n",
    "\n",
    "    # Code used to extract text from headers (h1, h2, etc.)\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    header_tags = [Tag(name='text') for h in headers]\n",
    "    for i, h in enumerate(headers):\n",
    "        header_tags[i].string = h.get_text(strip=True)\n",
    "        header_tags[i].level = 0\n",
    "\n",
    "    # Code used to extract text from paragraphs\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text_tags = [Tag(name='text') for p in paragraphs]\n",
    "    for i, p in enumerate(paragraphs):\n",
    "        text_tags[i].string = p.get_text(strip=True)\n",
    "        text_tags[i].level = 0\n",
    "\n",
    "    # Code used to extract text from lists\n",
    "    lists = soup.find_all(['ul', 'ol'])\n",
    "    for lst in lists:\n",
    "        text_tags.extend(process_list_items(lst))\n",
    "\n",
    "    # Code used to sort the tags based on their order of appearance on the website\n",
    "    all_tags = header_tags + text_tags\n",
    "    all_tags.sort(key=lambda x: x.previous_element.index(x) if x.previous_element else 0)\n",
    "\n",
    "    return '\\n'.join(tag.string for tag in all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cabf34",
   "metadata": {},
   "source": [
    "### Function that Processes Text Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d3e4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_with_spacy(text):\n",
    "    # Code used to disable spaCy's named entity recognition (NER) component\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "    doc = nlp(text)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486d775",
   "metadata": {},
   "source": [
    "### Function that Extracts Text Processed Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cdf06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_spacy(doc):\n",
    "    return doc.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714eac0",
   "metadata": {},
   "source": [
    "### Function that Converts the Extracted Text to DOCX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc87e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_docx(text, output_path=\"output.docx\"):\n",
    "    doc = Document()\n",
    "\n",
    "    # Code used to split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        # If statement used to skip empty lines\n",
    "        if line.strip():\n",
    "            # If statement used to check if the document already has content\n",
    "            if doc.paragraphs:\n",
    "                # Code to add a new paragraph for each additional line\n",
    "                doc.add_paragraph(line)\n",
    "            else:\n",
    "                # Code used to add the first line without an additional paragraph\n",
    "                doc.add_paragraph(line, style='BodyText')\n",
    "\n",
    "    doc.save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c5e4c",
   "metadata": {},
   "source": [
    "# Text to DOCX Converter\n",
    "Finally, we have our code that allows for text from PDFs, Image-Only PDFs, and Websites to be extracted and sent to Word Document in DOCX format. In this Notebook, I will demonstrate how to extract text from a PDF simply by running this code. Everyday benefits of this are for instances where information from massive multi-page PDFs must be copied in order to generate reports using an application like Microsoft Word. Normally, copying text in this nature would involve simply highlihgting the text, and then copying and pasting it into a Word document of your own creation. However, for larger documents with 100+ pages, for example, there is a risk of one's grip on the left mouse button slipping and thus eliminating all of their progress. This aspect of the code was created for this purpose in order to save time copying and pasting content from highlightable PDFs and eliminate frustration. Running this on a local machine is helpful when dealing with confidential documents.\n",
    "\n",
    "Using this converter is simple, as all one has to do is load the path to their PDF in the \"pdf_path\" variable. For this demonstration, I have selected the textdemo.pdf file via the path it exists in on my local machine. Next, one simply must define the \"output_path\" as the location they want their outputted DOCX file to be in. In my case, I named it \"textdemo.docx\" and decided to store it in the same folder as my original PDF. Also worth noting, when not using the Website extraction feature, it is best to just keep the path to that assigned as 'https://none.com'.\n",
    "\n",
    "With all of that in mind, simply run the code and the text from the PDF is extracted to a DOCX file. In this case, the original PDF contains text that reads, \"This PDF contains text for demonstration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a290d103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved to C:\\\\projects\\\\textdemo.docx\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r'C:\\\\projects\\\\textdemo.pdf'  # Change this to your PDF file path\n",
    "    website_url = 'https://none.com'  # Change this to the desired website URL in https://example.com format if you wish to extract text from websites\n",
    "\n",
    "    # Code used to extract text from PDF\n",
    "    extracted_text, has_images = pdf_to_text(pdf_path)\n",
    "\n",
    "    # Code used to extract text from website\n",
    "    if website_url:\n",
    "        website_text = extract_text_from_website(website_url)\n",
    "        extracted_text += \"\\n\" + website_text\n",
    "\n",
    "    if has_images:\n",
    "        # Code used to extract text from images\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc[page_num]\n",
    "            if is_image_page(page):\n",
    "                image = page.get_pixmap()\n",
    "                image_text = extract_text_from_image(np.frombuffer(image.samples, dtype=np.uint8).reshape((image.h, image.w, image.n)))\n",
    "                extracted_text += \"\\n\" + image_text\n",
    "\n",
    "        doc.close()\n",
    "        \n",
    "\n",
    "    # Process the text with spaCy\n",
    "    spacy_doc = process_text_with_spacy(extracted_text)\n",
    "    final_extracted_text = extract_text_spacy(spacy_doc)\n",
    "\n",
    "    # Output to Word document\n",
    "    output_path = r'C:\\\\projects\\\\textdemo.docx'  # Change this to your desired output path\n",
    "    text_to_docx(final_extracted_text, output_path)\n",
    "\n",
    "    print(f\"Text extracted and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196c423",
   "metadata": {},
   "source": [
    "And now so does the DOCX file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec8cc3",
   "metadata": {},
   "source": [
    "# Reading the Extracted Text Without Opening the Document\n",
    "As a bonus, I wanted to write some code that may be used to print the extracted text for exemplary purposes. Good reasons to do this may be that you simply want to examine the contents of the extracted DOCX file without the need to actually open it. This is useful if you happen to be running this on a machine that does not have Microsoft Word installed. By that logic, you could use this code to validate its contents and then email the DOCX file to a machine that has the necessary software to edit it. For a scenario like this, I recommend using Google Docs as DOCX files can be uploaded there if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dae52237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: This PDF contains text for demonstration. \n",
      " \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracted Text:\", extracted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
